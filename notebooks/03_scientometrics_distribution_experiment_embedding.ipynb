{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f01a8400-5267-4dd5-a990-dee8d8f666a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2934bfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.6\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4363850a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma OK: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "# test chroma\n",
    "print(\"Chroma OK:\", chromadb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be475d96",
   "metadata": {},
   "source": [
    "### 1. Prepare paths and localisation\n",
    "\n",
    "Ensure the project root is the working directory so that relative paths\n",
    "and imports from the `src` package behave consistently with .py scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6f01a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zmieniono katalog roboczy na: d:\\$projects\\python\\master_thesis\n"
     ]
    }
   ],
   "source": [
    "if os.getcwd().endswith('notebooks'):\n",
    "    os.chdir('..')\n",
    "    print(f\"Zmieniono katalog roboczy na: {os.getcwd()}\")\n",
    "\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229994f2",
   "metadata": {},
   "source": [
    "### 2. Generating a Global Scaler\n",
    "\n",
    "Before running the simulation, it is crucial to create a **global scaler** file (`.pkl`) to ensure consistent feature scaling across the entire project. This step guarantees that popularity weights and other features remain comparable across experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b1719a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health Check - Scaler: Global Scaler saved to models/global_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the intermediate dataset\n",
    "df_interim = pd.read_csv('data/interim/articles_with_score_df.csv')\n",
    "\n",
    "# 2. Log-transform citation counts\n",
    "#    Prevents records with extremely high citations from dominating the scaling process\n",
    "df_interim['n_citation_log'] = np.log1p(df_interim['n_citation'])\n",
    "\n",
    "# 3. Fit a MinMaxScaler on the global features\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df_interim[['year', 'n_citation_log', 'gov_score']].values)\n",
    "\n",
    "# 4. Save the scaler to the models folder for later use\n",
    "os.makedirs('models', exist_ok=True)\n",
    "joblib.dump(scaler, 'models/global_scaler.pkl')\n",
    "\n",
    "print(\"Health Check - Scaler: Global Scaler saved to models/global_scaler.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02150f0c",
   "metadata": {},
   "source": [
    "### 3. Declare Experiment Settings\n",
    "\n",
    "Experiment settings are stored as a **list of dictionaries**. Each dictionary represents one configuration of parameters for the simulation, including dataset size, number of citations, and feature weights. This setup allows systematic testing of multiple combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37afea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example configurations (you may have 300+)\n",
    "# settings = [\n",
    "#     {\"N\": 10, \"k\": 5, \"pn\": [0.25, 0.25, 0.25, 0.25]}, # Equal distribution of weights\n",
    "#     {\"N\": 10, \"k\": 5, \"pn\": [1.0, 0.0, 0.0, 0.0]},     # Only semantic similarity\n",
    "#     {\"N\": 100, \"k\": 10, \"pn\": [0.0, 0.0, 1.0, 0.0]}    # Only citations (N=100)\n",
    "# ]\n",
    "\n",
    "# # Optionally, save the experiment settings to a file\n",
    "# # This allows other modules or experiments to read the configurations consistently\n",
    "# os.makedirs('data/external', exist_ok=True)\n",
    "# pd.to_pickle(settings, 'data/external/settings.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c67ef24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config.settings_generator import generate_all_settings\n",
    "\n",
    "SETTINGS_PATH = \"data/external/settings.pkl\"\n",
    "settings = generate_all_settings()\n",
    "\n",
    "os.makedirs('data/external', exist_ok=True)\n",
    "pd.to_pickle(settings, SETTINGS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9737d214",
   "metadata": {},
   "source": [
    "### 4. Fragmentary Smoke Test of the Engine\n",
    "\n",
    "Before running a full batch of 40,000 queries, it is recommended to perform a **smoke test**. This ensures that the `Experiment` class correctly initializes the `VirtualAggregator` and can process at least one query without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0471c5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-28 14:04:41,783 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-12-28 14:04:41,886 - INFO - Loading queries from data/interim/queries_with_embeddings.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting 0 ---\n",
      "Liczba wybranych unikalnych prac: 5\n",
      "Top 3 najczęściej wybrane ID: [(np.str_('535037'), 1), (np.str_('745295'), 1), (np.str_('4315'), 1)]\n",
      "\n",
      "\n",
      "--- Setting 1 ---\n",
      "Liczba wybranych unikalnych prac: 5\n",
      "Top 3 najczęściej wybrane ID: [(np.str_('387008'), 1), (np.str_('494046'), 1), (np.str_('841109'), 1)]\n",
      "\n",
      "\n",
      "--- Setting 2 ---\n",
      "Liczba wybranych unikalnych prac: 10\n",
      "Top 3 najczęściej wybrane ID: [(np.str_('491641'), 1), (np.str_('464010'), 1), (np.str_('791746'), 1)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgrammingLang\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\ProgrammingLang\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\ProgrammingLang\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example: Run a single-query smoke test\n",
    "# This verifies that all components (embedding, aggregation, scoring) are wired correctly\n",
    "\n",
    "from src.models.experiment import Experiment\n",
    "\n",
    "\n",
    "# Initialize the experiment with minimal settings\n",
    "settings = pd.read_pickle(\"data/external/settings.pkl\")\n",
    "\n",
    "exp = Experiment(settings) # test_mode=True\n",
    "\n",
    "# Process a single query to validate the pipeline\n",
    "single_result = exp.run_single_query(0)\n",
    "\n",
    "for s_id, distribution in single_result.items():\n",
    "    print(f\"--- Setting {s_id} ---\")\n",
    "    print(f\"Liczba wybranych unikalnych prac: {len(distribution)}\")\n",
    "    print(f\"Top 3 najczęściej wybrane ID: {distribution.most_common(3)}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892cdbd8",
   "metadata": {},
   "source": [
    "### 5. Run main step\n",
    "\n",
    "This step launches the full simulation. Thanks to the **built-in health check** in the `Experiment` class, the pipeline can automatically resume from the last saved query if the process was interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb7a71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 05:59:07,447 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-12-31 05:59:07,595 - INFO - Starting experiment execution\n",
      "2025-12-31 05:59:07,595 - INFO - Loading queries from data/interim/queries_with_embeddings.pkl\n",
      "2025-12-31 06:03:44,851 - INFO - Skipping already processed queries: 800000\n",
      "2025-12-31 06:03:44,851 - INFO - Processing query range: 800000–850000\n",
      "Queries: 100%|██████████| 50000/50000 [2:03:53<00:00,  6.73it/s]   \n",
      "2025-12-31 08:07:38,050 - INFO - Final result persistence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment batch finished.\n"
     ]
    }
   ],
   "source": [
    "from src.models.experiment import Experiment\n",
    "\n",
    "# Configure the experiment orchestrator\n",
    "settings = pd.read_pickle(\"data/external/settings.pkl\")\n",
    "\n",
    "experiment_orchestrator = Experiment(settings)\n",
    "\n",
    "# The batch parameter defines how many queries to process in this session (e.g., 40,000)\n",
    "# Checkpoints (CSV saves) are automatically performed every 500 queries (configured in the class)\n",
    "experiment_orchestrator.run_experiment(batch=50000)\n",
    "\n",
    "print(\"Experiment batch finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
